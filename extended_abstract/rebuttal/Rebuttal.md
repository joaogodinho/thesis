We would like to thank the reviewers for their constructive reports on our work and will briefly comment below on the raised issues. We will proceed through the remarks presented in each review, merging some of them whenever that makes sense.


- On the public availability of datasets (R1): the data we used for our analysis was collected from 3 different online sources: Malwr.com, NSRL, and VirusShare.com. Although these can be freely used for research, it is not clear from their terms of usage if we could make them available online. We believe however that it would be possible to make them privately available for analysis of the reviewers.

- On the advantages of the stacking methodology (R1): Although not presented in the paper, the stacking methodology presented a slight advantage in terms of accuracy when compared to the linear regression counterpart (~2%). As a side effect of this, we also tackle another important issue in malware classification that is "naming" and division in classes. Even if the end-user of an IDS/IPS is mainly concerned if a given sample is malware or goodware, other users, like researchers, benefit from a more detailed naming, as it helps to analyse and comprehend threats. For this reason, the stacking methodology does give advantages, even under marginal performance advantages.

- Keeping a small set of recent samples vs constant retraining of the model (R1+R3): In our work we were more interested in analysing how much data would be needed to keep a high degree of accuracy. It would not be totally unexpected that to achieve that, the size of the training dataset would constantly increase making this analysis potentially infeasible in a few years time. What our results show is that, for this particular dataset, the compromise that has to be made in terms of accuracy in order to keep a relatively small training dataset is not significant.
The proposed approach by the reviewer would be an interesting one and in fact iteration 2 of Past-to-Present (Fig8) would be the last iteration of that process. One can then see in Figure 14, green line, that although we are just looking at the results of one iteration, the accuracy of this iteration is less than the sliding-window approach.
So, although this comment is valid, and given that the cost of training using the sliding-window is <1min, we believe that the advantages of the sliding-window approach compensate the "low-cost" of the retraining.

- Chosing the "reliable" vendors (R1): Our goal was to select the top-vendors according to each metric (Fig 3 and Fig5). Using the form $mx + b = y$ enables us to balance the weight of FPR over DR. By increasing the slope (i.e. $m>1$) we prioratize low FPR over DR, whereas by decreasing the slope (i.e. $m<1$) we prioratize higher DR over FPR. By letting $m=1$ we did not prioritize any of the solutions.

- As for the organizational aspect mentioned by R1 and R3, we appreciate these comments that will be incorporate in a final version of this paper. Similarly for the images. While we tried to incorporate justifications within the text, some work can be done to improve their readability.

- On the sufficiency of static imports (R2+R3): due to the nature of malware, and the usage of run-time obfuscation techniques, it is conceivable that dynamic analysis significantly outperforms static analysis. In our work, we were interested in analysing how much one could get just by simple analysis of static imports. Our claim is that the simplicity and speed of this analysis is a factor that should be taken into consideration, as the accuracy results show. Also, for low latency applications, such as inline with other email scanning tools, time is a constraining factor, thus static analysis presents itself as a viable option. Of course, combining this with dynamic analysis should only provide better results, although compromising (time) performance.

- The differences Pf-Pl (R2): if Pf-Pl>0, it means that the number of malware classifications reduced, which means that, as a community, we are in the presence of a False Positive. Some vendors classified a sample as malware and then changed their classification (or there was some changes in both directions with a "net-value" in that direction). Symmetrically for Pf-Pl<0. Of course when Pf=Pl, the "positive-vendors in P_first" may be completey disjoint of the "positive-vendors in P_last" but, as a community, the sample did not change its valuation. We have thus an analysis on how the community changed their classification (Figure 2). This curve is not symmetric, which underpins the idea that they do not change the classification in a symmetric fashion. Nevertheless, this fact would not change the statistical meaning of the assessment.

- Why 5 vendors (R2+R3): In Figure 2 one can see that the trend to change valuations from goodware to malware (green bars) is much bigger than the opposite (red bars). Moreover, if we consider the integral up to -5, we get that that value is <0.05%. 
This tells us that less than 5 vendors change their classifications from malware to goodware in 99.5% of the times. 
The reasoning behind this choice for cut-off was that if 5 (or more) vendors classify a sample at the beginning as malware, then in 99.5% of the cases it will still be classified as malware in the final evaluation. 

- Distinction between upper case and lower case imports (R2); In fact we tried to normalize the imported library names and realized that we obtained worse results when ignoring case sensitivity. We cannot assert the reason behind it, but it could be possible that goodware development usually resorts to a lot of stacked frameworks and libraries, whereas malware is usually done in a more ad-hoc way, hence it may use less libraries (or less common ones).

- (R2) Due to the categories not being well defined and changing over time, regular machine learning validations should be used with care, as we have shown, obtaining good values is not hard, when no temporal order is imposed. This does not invalidate the use of machine learning as a whole, if proper precautions are taken such as imposing temporal consistency, since we are off-loading manual analysis work from malware analysis.

- Comment on AUROC (R2): This work aims at providing a comparative analysis on which approach fits better malware detection, rather than providing the best model or architecture for malware detection. In this sense, the results should be taken relatively (to each other) and not absolutely: we aim at comparing one approach with another using the same (certainly not the best) model throughout the paper, so that we can draw some comparative conclusions. For this reasons, we do not intend to emphasize the impresiveness of an AUROC above 0.9, but rather its meaning when compared with the remaining approaches. Ideally one could change the model under our defined scenarios and obtain the same relative values between scenarios.

- On the choice of NSRL and VirusShare and identification of samples (R3): we used these two sources as the former is a clear source of goodware, and the latter a clear source of malware. This allowed us to clearly classify some of our samples as good/malware.
Also, in both these repositories, and on malwr.com, samples are identified by the MD5 hash of their payload. This allowed us to not only validate which samples from malwr.com are good/malware but also which samples correspond to repeated submissions.

- 5.56% decrease corresponds to the integral of the red bars in Figure 2. Justified in an earlier comment.

- The analysis of vendors' FP, FN, TP, TN rates stands on their classifications compared against our definition of malware (5+ positive) and goodware (0 positive). We are not sure on what does R3 means with "FT_v". If it means that (exactly) 2 vendors classify a sample positively, that sample would not fit our definitions of malware/goodware and would not be contained in our training example, due to the uncertainty underlying (classical) malware detection.

- The purpose of the table on page 5 was to show the size of the datasets.

- Discussion of proposed evaluation metrics (R3): Cross-validation is a standard ML model but not quite applicable to the context of malware detection due to the temporal order underlying a (real) malware detection scenario where samples are observed in a given order. For this reason, we have proposed the temporal-based models. In the temporal-based scenario, we have analysed the impact of both reducing and sliding the training set, and drew some conclusions on how we could minimize the training data without compromising the optimal values.

- time spent for the training and for the validation: answered above. <1min for sliding window.
