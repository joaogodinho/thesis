\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{todonotes}
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{dirtytalk}
\usepackage{color}
\usepackage{glossaries}

% Reset footnotes every page
%\usepackage{perpage}
%\MakePerPage{footnote}

\begin{document}
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%  HEADER  %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\title{Malware Detection Via Machine Learning}
\subtitle{Some Clever Subtitle?}
\titlerunning{Malware Detection Via ML}
\author{João F. Godinho\inst{1} \and Pedro Adão\inst{2}}
\institute{Instituto Superior Técnico\\
\email{enter email}\and
Instituto Superior Técnico\\
\email{enter email}}
\maketitle
\todo[inline]{Validate the title, subtitle, authors, email}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% ACRONYMS %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\newacronym{caro}{CARO}{Computer Antivirus Research Organization}


%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% ABSTRACT %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The abstract should summarize the contents of the paper
using at least 70 and at most 150 words.
\end{abstract}



\clearpage
\todo[inline]{Table of contents?}



\mainmatter
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%  INTRO   %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}
% #1 Semantics-Aware Malware Detection
% #2 Attacking Malicious Code: A report to the Infosec Research Council
% #3 Malware, Rootkits & Botnets A Beginner's Guide pp.10
% #4 Computer viruses: theory and experiments
% #5 Theory of self-reproducing automata
% #6 The Evolution of Viruses and Worms
% #7 The art of computer virus research and defense
% #8 https://www.av-test.org/en/
% #9 http://www.caro.org/articles/naming.html
% #10 http://members.chello.at/erikajo/vnc99b2.txt
Malware, which can be referred to by other names like malicious software or malicious code, is not easily described by a single definition. For example, Christodorescu et al.[quote 1] simply describe malware as \say{a program that has malicious intent}, which in turn begs the question, what is malicious intent? McGraw and Morrisett[quote 2] give a more concrete definition by saying \say{Malicious code is any code added, changed, or removed from a software system in order to
intentionally cause harm or subvert the intended function of the system}. Both definitions give an overall understanding of \textcolor{red}{(on?)} what is malware, but to better understand the definition, one can briefly look at the history of malware.

\todo[inline]{Clean previous paragraph. Remove bellow}

The term malware was first used by Yisrael Radai in 1990[quote 3], before then malicious software was referred to as computer viruses, a notion which was first formalized by Cohen in 1983[quote 4]. Before Cohen, John Von Neumann had already done some similar academic work that closely relates to computer viruses, namely \say{Theory of self-reproducing automata}, published in 1699[quote 5]. The term computer virus is one of the many types of malicious software, but given that it predates the term malware, it is not uncommon to see both terms used as interchangeably.

Even before the formalization of computer virus by Cohen in 1983, one of the first documented computer virus, although experimental, was developed by Bob Thomas at BBN in 1971[quote 6]. The program was named Creeper and consisted of a self-replicating program that would print a message and move between the nodes of ARPANET.

Another malicious software worth mentioning, given it was one of the first known virus that spread \say{in the wild} (i.e. outside the computer system or laboratory in which it was developed), is Elk Cloner. Created by Rich Skrenta in 1982[quote 7], Elk Cloner spread by floppy disk and had a payload that displayed Skrenta's poem after every 50th use of the infected system.

\todo[inline]{Up to here}

Since then, the amount of malware has grown at an alarming rate. According to AV-TEST[quote 8], an independent security software group, up to 2012 the amount of total malware per year would not surpass 100 million, whereas 2013 almost reached 200 million, followed by more than 100 million increase per year, with almost 600 million total malware before the end of 2016.

\todo[inline]{Clean previous paragraph. Remove bellow}

Even though the term malware generalizes the malicious intent of a piece of software, one can classify malware further based on its purpose and behavior. Given the high amount of malware, there is also a high variety of malware, based on purpose and behavior.

From the general purpose and behavior of a malware sample, it can be classified as belonging to one or more malware categories (or classes). Examples of categories are virus, worms, trojans and adware, to name a few. Having the ability to classify malware into a more restrict subset is helpful, but there is still a need to distinguish between samples of the same category, by giving more detailed names to malware.

One attempt at creating a naming convention was made in 1991, by the Computer Antivirus Research Organization (CARO). The convention states the name should consist of four parts, separated by a dot. The four constituents are the family name, group name, major variant name and minor variant name, displayed in that order. An optional modifier could also be appended to the name with a colon. The problem with the aforementioned convention is that it only covered computer viruses, which were more prominent at the time.

An attempt at improving CARO's naming convention was made in 1999, by Gerald Scheidl[quote 10]. His proposal added two other constituents, platform and type, as a prefix to CARO's naming.

Later attempts to standardize malware naming were made, none of which resulted in a standard widely adopted. Due to this, antivirus vendors use their own naming scheme, which are variants of the mentioned schemes.

Since the malware naming scheme is not standardized, and given the amount and variety of malware, different family names with the same behavior emerge from different antivirus vendors. The name redundancy is worsen by how different vendors detect and classify malware.

With the raise of malware amount and diversity, being able to identify and classify malware becomes essential in order to protect systems.

Before the late eighties, given the low amount of malware, the first antivirus programs were designed to find and remove a handful of malware, mainly through signature based methods, which detect malware by looking for specific patterns that appear in known malware.

In the late eighties and nineties the first antivirus were released, along with more sophisticated detection methods that make use of heuristics, which also detect malware by also looking for patterns, but at a higher level of abstraction.

In more recent years, and given the increment of malware amount and variety, signature and heuristic methods fall short when keeping up with new types of malware. The mentioned methods work best for samples of known malware or new malware with small variants of known malware. This is because these methods look for known patterns. If a new type emerges, which pattern's are not known, it is not flagged as malicious.

Given this limitation, more advanced methods like behavioral analysis are applied together with the previous mentioned methods. Behavioral analysis takes into account the actions performed by the malware, introducing dynamic information, from which patterns can be extracted and again matched against known patterns.

Having to run samples in order to extract dynamic information poses the problem of infecting the system running the samples. One way to prevent this is to use a sandbox environment, which is a security mechanism to isolate running programs. Using a sandbox environment closely relates to computer virtualization.

To provide users the possibility to check files without running the risk of being infected, online services allow users to submit files which are then run in a sandbox environment.

An example of such service is malwr. Malwr takes user submissions and runs them in a sandbox environment, specifically Cuckoo Sandbox, providing static and dynamic information about the sample, as well as checking if the file is known in VirusTotal, a similar service that also checks if the submission is known in different antivirus solutions.

% VirusScan (from McAfee) and NOD (from ESET) were first released in 1987; AntiVir (from Avira) and Avast Antivirus were released in 1988. Norton Anti-Virus (from Symantec) and F-Secure antivirus were released in 1991. 

\todo[inline]{re-focus on high amount of malware, difficulty of having to run all samples and check}
\todo[inline]{propose machine learning as a solution to deal with the high amounts of data}
\hfill \break


\todo[inline]{Motivation}
\begin{itemize}
	\item Define Malware, problem with that, subjectivity
	\item Quantify malware in recent years (quote vendors/get stats from header analysis)
	\item What is being done about malware?
	\item How is malware identified?
	\begin{itemize}
		\item Types of strategies
		\item Is it effective? efficient?
	\end{itemize}
	\item Being able to identify malware as early as possible would help?
	\item Introduce services like malwr.com/virustotal.com
	\item Introduce Machine Learning capabilites
	\item Automation of detection/classification
\end{itemize}

%From an external view, adware and ransomware will provide a noisy output to the user, given their function to deliver unwanted advertisements (for adware) and request data ransom (for ransomware). Spyware and rootkit types, on the other hand, will provide little to no user interaction, given their function to spy on user activity (for spyware) and conceal processes, files or system data (for rootkit).
%
%From an internal view, adware and ransomware will behave similarly, since both need interfaces that facilitate user interaction (e.g. create and show dialogs), in contrast to spyware and and rootkit, which thrive on concealing actions (e.g. run hidden processes).

% Quote this for more types of malware
% https://support.symantec.com/en_US/article.HOWTO54185.html



\todo[inline]{Problem}
\begin{itemize}
	\item Remove the human factor from detection/classification to speed up process.
	\item Introduce lack of temporal consistency here?
	\item Talk how related work creates own corpus and low corpus size/variation?
	\item Refer stats from header analysis? Time vendors take to take knowledge of samples/Time until new malware is detected in most vendors
	\item Reinforce time to classify malware?
\end{itemize}

\todo[inline]{Hypothesis}
\begin{itemize}
	\item Would a more automated approach, based on ML, facilitate malware detection/classification?
	\item Dynamic/Static analysis to detect and classify malware
	\item Temporal consistency is a factor that should be taken into account?
	
\end{itemize}
\todo[inline]{Objectives/Contributions}
\begin{itemize}
	\item Usage of real world analysis
	\item Try to apply ML to PE32 analysis from malwr.com (Talk about this here?)
	\item Contribute with a corpus of analysis from malwr.com
	\item Maintain a temporal consistency
		
	
\end{itemize}
\todo[inline]{Document outline}
The following sections are organized as follows. Section \ref{sec:related_work} describes related work and its influence on the current work. Section \ref{sec:solution} describes the planned approach for solving the problem at hand. Section \ref{sec:conclusion} closes the report and references the work done so far.

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% RELATED  %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}\label{sec:related_work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Background
\subsection{Background}\label{subsec:background}
The term malware was first used by Yisrael Radai in 1990[quote 3], before then malicious software was referred to as computer viruses, a notion which was first formalized by Cohen in 1983[quote 4]. Before Cohen, John Von Neumann had already done some similar academic work that closely relates to computer viruses, namely \say{Theory of self-reproducing automata}, published in 1699[quote 5]. The term computer virus is one of the many types of malicious software, but given that it predates the term malware, it is not uncommon to see both terms used as interchangeably.

Even before the formalization of computer virus by Cohen in 1983, one of the first documented computer virus, although experimental, was developed by Bob Thomas at BBN in 1971[quote 6]. The program was named Creeper and consisted of a self-replicating program that would print a message and move between the nodes of ARPANET.

Another malicious software worth mentioning, given it was one of the first known virus that spread \say{in the wild} (i.e. outside the computer system or laboratory in which it was developed), is Elk Cloner. Created by Rich Skrenta in 1982[quote 7], Elk Cloner spread by floppy disk and had a payload that displayed Skrenta's poem after every 50th use of the infected system.

Over the years, malware has grown in size and diversity, which introduces the need to be able to distinguish malware, not only to facilitate its removal, but also to provide safer mechanisms of defense.

Based on the malware general purpose and behavior, a high level classification of the malware can be made by splitting malware into one or more categories (or classes). Examples of such classes are viruses, worms, trojans and adware, to name a few. This high level classification is helpful, but lacks detail, like target platform, origin or version. In order solve this issue, attempts have been made to create a naming convention for malware.

One of the first attempts was made in 1991, by the \gls{caro}\footnote{http://www.caro.org/articles/naming.html}, which states the name should consist of four  obligatory parts, separated by a dot, and an optional fifth part. The four main constituents are the family name, group name, major variant and minor variant name. The optional fifth part is a modifier and could be appended to the name with a colon.
\todo[inline]{Describe the parts?}

Although the aforementioned naming convention is well defined, it is aimed at naming viruses, lacking differentiation for other classes as well as target platform. In order to solve this, an improvement was proposed in 1999 by Gerald Scheidl\footnote{http://members.chello.at/erikajo/vnc99b2.txt} that suggested the addition of a prefix that consisted of the platform and type.

A more recent attempt at creating a naming convention was made during the Virus Bulletin Conference in 2001\footnote{https://www.virusbulletin.com/uploads/pdf/magazine/2002/200201.pdf}, with the idea that malware naming should consist on three levels of information: malware name, classification and full text description.
\todo[inline]{Again, describe parts?}

None of the referenced naming conventions is widely adopted, with antivirus vendors having their own naming conventions, that although similar, vary in some way or another. For example, Trend Micro\footnote{http://docs.trendmicro.com/all/ent/smex/v11.0/en-us/smex\_11.0\_olh/secrisk\_malware\_name.html} and Symantec\footnote{https://www.symantec.com/security\_response/virusnaming.jsp} use a prefix (which identifies the malware class), threat name (i.e. family) and suffix (which indicates a variant of the threat). Whereas Microsoft\footnote{https://www.microsoft.com/security/portal/mmpc/shared/malwarenaming.aspx} tries to follow the \gls{caro}'s convention, by using a type, platform, family, variant and information. Avira\footnote{https://www.avira.com/en/support-malware-naming-conventions} on the other hand, takes a more simplistic approach by using a prefix (which identifies the malware type) and a threat name.

Although the naming and its constituents are similar between vendors, no convention is set, hence the threat names given to malware varies, increasing the redundancy for the same malware. According to an article published by Symantec in 2002\footnote{https://www.symantec.com/connect/articles/virus-any-other-name-virus-naming-practices}, based on a list of 300,000 unique names, the name redundancy is quantified as an average of four different names for the same malware.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Malware detection
\subsection{Malware Detection}\label{subsec:malware_detection}
In the early days of malware, due to low amount and variety, antivirus could target malware specifically (e.g. Reaper was created to remove Creeper), but with an estimate of more than 600 million total malware by the end of 2016, targeting specific malware becomes increasingly more difficult, hence being able to identify and classify malware efficiently is crucial to protect systems.

What follows is a quick rundown on different detection techniques developed over the years[quote 7].

\subsubsection{First-Generation Scanners.} These methods of detection work at a fairly simple level and use static methods of detection. The applied principles include:
\begin{itemize}
	\item String scanning, where a sequence of bytes that is common in a malicious program but not in a regular program is matched against the suspicious file.
	\item Wildcards, similar to string scanning, but instead of a fixed sequence of bytes, regular expressions can be used in the sequence of bytes, this allows variation and byte skipping when matching.
	\item Mismatches, also similar to string scanning, in which a sequence of bytes is taken as reference, but a number of bytes $N$ is given allowing up to $N$ bytes to vary in any position of the string (e.g. the string \say{\texttt{01 02}} with $N=1$ will match \say{\texttt{XX 02}} or \say{\texttt{01 XX}}).
\end{itemize}

The described principles can be applied with slight variations that improve the performance of the scanner, such as matching on the beginning and end of the file (top-and-tail scanning) or matching at a certain offset (bookmarks).

They all require a knowledge base with known patterns that must be kept up-to-date in order to detect newer malware.

\subsubsection{Second-Generation Scanners.} These methods improve on the previous generation by doing a static analysis at a higher level of abstraction. Where first-generation scanners would match sequence of bytes, second-generation scanners also look at the representation of the bytes. The applied principles include:
\begin{itemize}
	\item Smart scanning, where the Assembly representation of the bytes is taken into account and instructions that have no effect (e.g. \texttt{NOP}) can be skipped. The resulting signature is smaller and more easily detects mutations. The same principle could be applied to textual malware (e.g. scripts), where extra white spaces have no effect.
	\item Nearly exact identification, where instead of relying on a single string for detection, double-string detection can be used. An alternative to double-string detection could also be based on a checksum range selected from the malware body, giving a higher accuracy by taking into account a larger portion of the malicious file.
	\item Exact identification, which can exactly identify malware variants, is usually used in combination with first-generation methods and instead of using a single range like nearly exact, uses multiple ranges to calculate checksums. To be accurate, this method must only take into account constant bits of the malicious file, and given that more ranges are used it is slower then the previous ones.
\end{itemize}

Like in the previous generation, a knowledge base must be used and kept up-to-date to identify newer malware.

\subsubsection{Algorithmic Scanning Methods.} These methods are not so focused on direct matching of signatures, but on the routines implemented in the scanner that allow the detection of malware. Early implementations of algorithmic scanning were hard-coded in the antivirus, but with different types and families emerging, the hard-coded algorithms could easily become obsolete as such. To overcome this, vendors would introduce more specific routines for malicious programs when updating the antivirus.

Being able to update the antivirus algorithms frequently enables the vendors to have a faster response to new malware by developing detection routines for new malware and pushing them to end users.

\subsubsection{Code Emulation.} This detection technique uses dynamic analysis to detect malware by simulating code execution. A virtual machine is used to simulate the system's components, thus the malicious files are encapsulated and no code is executed by the real processor.

Code emulation can be faster than using algorithmic scanning methods when dealing with encrypted/obfuscated malware, as the code can be emulated and the decrypted/deobfuscated code is much easier obtained.

\subsubsection{Heuristic Analysis.} This detection technique takes into account several characteristics of malicious software, both static and dynamic. Examples of such characteristics are code execution starting in uncommon sections, suspicious code redirection or incorrect size of code in header.

When checking a suspicious file, the presence or absence of multiple characteristics is weighted to assert if the file is deemed malicious or not. The upside of this technique is that it allows for never seen malware to be correctly flagged, but brings the downside of a higher amount of false positives.

\subsubsection{Sand-Boxing.} This detection technique builds on the already mentioned code emulation, but with a higher level of complexity. With sand-boxing the whole system is emulated (i.e. virtualized) and suspicious or malicious programs have access to a copy of the real resources, effectively limiting the effects of the program and consequently protecting the system.

This technique allows to extract dynamic information without compromising the real system, which can then be used to both detect if a suspicious program is malware, by applying the aforementioned techniques, and to help analyze new types of malware.

The downside of using sand-boxing comes in the form of compatibility issues and in the possibility of malware breaking out of the sand-box due to vulnerabilities in the virtualization software.\\

Detecting malware is not as simple as applying one of the mentioned techniques, different methods work best in different scenarios, hence antivirus need to balance the usage of these techniques to keep systems protected.

\todo[inline]{Talk about cuckoo here?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Machine Learning
\subsection{Machine Learning}

\todo[inline]{Paragraph introducing ML?}
\todo[inline]{quick rundown on supervised/unsupervised/semi-supervised}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Machine Learning in malware detection
\subsection{Machine Learning in Malware Detection}

% #11 Reviewer Integration and Performance Measurement for Malware Detection

Miller et al provide an interesting work on combining machine learning techniques with manual review of samples[quote 11]. They extract both static and dynamic features from binaries and apply logistic regression to achieve a 72\% detection rate with a 0.5\% false positive rate. They also note that performance measurement can easily be inflated when ignoring temporal consistency in both binary samples and labels. Such can happen when using methods like \textit{cross-validation} for performance measurement, which implies that all types of malware are both seen in evaluation and in training, consequently the notion of unseen malware is not present.

% explicar as tecnicas e porque, como usa-las, vantagens, what's short
% 

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% SOLUTION %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution}\label{sec:solution}

With the rate at which malware is discovered, having the ability to automatically detect both known and new samples of malware facilitates the protection of systems, while reducing human interaction. As such, the current section proposes a solution that takes advantage of machine learning capabilities to detect malware, based on static and dynamic information that can be extract from malware samples.

The solution is divided into four sequential stages, where each stage directly depends on the previous one. The four parts are: collecting; parsing; training and validation. More details about each stage is described in the corresponding subsection.

With an implemented solution, it is expected to have a comparison on how different models predict a sample class (i.e. malware, not malware, what kind of malware), as well as benchmarks on the accuracy of the models.

\subsection{Collecting}
The first stage of the proposed solution, which consists on obtaining a corpus from which information about the malware can be extracted. For the purpose of this work, the obtained dataset consists on reports of static and dynamic information about binaries. Working with available reports instead of generating from samples saves the time overhead from both sample obtainment and report generation, allowing for more time better spent in the remaining stages, which are more related to the presented work.
\todo[inline]{Talk about using malwr reports}

\subsection{Parsing}
The second state of the proposed solution, that consists on parsing the previously obtained data. The reports must be parsed not only to filter relevant information, but also to guarantee data canonicalization. Having a well structured dataset eases feature extraction and posterior application of machine learning techniques, hence this stage is of extreme importance for the upcoming stage.
\todo[inline]{Talk about what can be extracted from this stage (metawise), like percentages, dups, etc}

\subsection{Training}
The third stage of the proposed solution, in which machine learning techniques are applied to obtain a model that can predict if a given sample is malicious and if so, what kind of malware is it. Before applying any machine learning techniques, a set of features must be selected from the dataset. Since the model will be trained based on the selected features, it is of uttermost importance that feature selection is done correctly, otherwise poorly chosen features will not be able to provide a good segregation between samples and consequently the final model will not as accurate.

\todo[inline]{talk about unsupervised methods to facilitate feature selection}
\todo[inline]{talk about thresholds for supervised methods to classify samples}

\subsection{Validation}
The fourth and last stage of the proposed solution, that consists on measuring how good the previously obtained model is. The validation of the model is done by taking a set of samples which were not used for training and predict to what class the sample belongs (i.e. malware, not malware, malware type). With the model prediction, and since the validation set is labeled, the true positive, true negative, false positive and false negative can be taken into account to assert how accurate the model is with regards to the correct labels.

\todo[inline]{Talk about using cross-validation as a baseline}
\todo[inline]{move to temporal consistency and how it performs vs cross}

\subsection{Tools (or some more relevant name?)}
Being able to easily modify and rerun each stage is one of the main concerns, as more time can be spent trying different techniques without spending much time on implementation. As such, Python 3 was chosen as the base language for development.

There are a large number of libraries in Python that provide easy access to machine learning capabilities, information retrieval and plotting and statistics. The ones that are of main interest for this work are: SciKit-Learn, Scrapy, Numpy, Pandas and MatPlotLib.

\todo[inline]{Talk here about INOV's machine?}

\subsection{Developed work}
The stages on which most time should be spent are training and validation. As such, some work as already been done in the previous stages, specifically in the collecting and parsing stage, so that more time can be dedicated to training and validation.

With regards to the collecting stage, ...

Regarding the parsing stage, ...

\clearpage

\todo[inline]{Start by the approach?}
\todo[inline]{Talk about the architecture}
\todo[inline]{Should evaluation and planning go inside this section?}
\todo[inline]{Talk about the stats of header analysis here?}
\todo[inline]{Talk about what has already been/being done?}
\begin{itemize}
	\item Begin with extraction from malwr.com
	\item Talk about why use of PE32?
	\item Create a corpus from the analysis
	\item How to classify samples as malware? How to define the threshold?
	\item How to do feature selection? Measure multiple features?
	\item Create a baseline to evaluate against, validate other's work? Ignore temporal consistency?
	\item Introduce temporal consistency in samples, how well one can detect/classify with limited samples
	\item Try to classify/detect samples that were taken after the training samples.
\end{itemize}

% tools existentes q vao ser usadas

% \section{•} % Metodologia de Avaliação do Trabalho
% \section{•} % Calendarização do Trabalho

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%CONCLUSION%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:conclusion}


\begin{thebibliography}{}

\end{thebibliography}

\end{document}