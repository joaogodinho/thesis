{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Malware classifier - Part 2\n",
    "\n",
    "The following uses the previous datasets to create a Logistic Regression classifier to predict if a sample is goodware or badware. The target sample to test is **bbb445901d3ec280951ac12132afd87c**, which is considered malware by 2% (1/48) of vendors on the first submission, evolving to 80%+ by the last submission.\n",
    "\n",
    "Each dataset includes samples split by submissions of the target sample (e.g. dataset0.csv includes submissions up to the first submission of **bbb445901d3ec280951ac12132afd87c**).\n",
    "\n",
    "Load all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 0th dataset size: 2798\n",
      "Final 1th dataset size: 31\n",
      "Final 2th dataset size: 22\n",
      "Final 3th dataset size: 121\n",
      "Final 4th dataset size: 286\n",
      "Final 5th dataset size: 465\n"
     ]
    }
   ],
   "source": [
    "import lib.data_loading as jcfg_data_loading\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dataset_name = \"dataset\"\n",
    "numb_datasets = 6\n",
    "\n",
    "datasets = []\n",
    "\n",
    "# Load all samples with an array, where each ith position are samples\n",
    "# up to the ith+1 submission of the target sample\n",
    "for i in range(numb_datasets):\n",
    "    datasets.append(pd.read_csv(dataset_name + str(i) + '.csv'))\n",
    "    datasets[i] = datasets[i].set_index('md5')\n",
    "    datasets[i].dropna(inplace=True, subset=['imports'])\n",
    "    print('Final {0}th dataset size: {1}'.format(i, len(datasets[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Transform the frames into binary vectors, where each position represents if a given import is present in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Features for 0th dataset: 10231\n",
      "#Features for 1th dataset: 10262\n",
      "#Features for 2th dataset: 10272\n",
      "#Features for 3th dataset: 10752\n",
      "#Features for 4th dataset: 11046\n",
      "#Features for 5th dataset: 11598\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the bag of words\n",
    "# Split by the semicolon\n",
    "count_vec_pattern = u'[^;]+'\n",
    "# A call must be present at least in x samples\n",
    "count_vec_min_df = 2\n",
    "# A call can't appear in more than x% of the samples\n",
    "count_vec_max_df = 0.1\n",
    "\n",
    "# percentage of train and validation\n",
    "training_size = 1.0\n",
    "validation_size = 1 - training_size\n",
    "\n",
    "count_vec_array = []\n",
    "\n",
    "# Create each bag\n",
    "for i in range(numb_datasets):\n",
    "    count_vec_array.append(\n",
    "        CountVectorizer(token_pattern=count_vec_pattern, max_df=count_vec_max_df, min_df=count_vec_min_df))\n",
    "\n",
    "# Create train and validation data\n",
    "# For each submission the training includes all the\n",
    "# previous submissions\n",
    "final_datasets = []\n",
    "\n",
    "# Create first one by hand, since it as no previous\n",
    "temp = train_test_split(datasets[0], test_size=validation_size)\n",
    "final_datasets.append({\n",
    "        'train_X': count_vec_array[0].fit_transform(temp[0].imports).toarray(),\n",
    "        'train_Y': temp[0].malware.values,\n",
    "        'validation_X': count_vec_array[0].transform(temp[1].imports).toarray(),\n",
    "        'validation_Y': temp[1].malware.values\n",
    "    })\n",
    "print('#Features for 0th dataset: {0}'.format(len(count_vec_array[0].vocabulary_)))\n",
    "\n",
    "for i in range(1, numb_datasets):\n",
    "    temp = train_test_split(pd.concat(datasets[:i+1]), test_size=validation_size)\n",
    "    final_datasets.append({\n",
    "        'train_X': count_vec_array[i].fit_transform(temp[0].imports).toarray(),\n",
    "        'train_Y': temp[0].malware.values,\n",
    "        'validation_X': count_vec_array[i].transform(temp[1].imports).toarray(),\n",
    "        'validation_Y': temp[1].malware.values\n",
    "    })\n",
    "    print('#Features for {0}th dataset: {1}'.format(i, len(count_vec_array[i].vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LR Parameters\n",
    "# Inverse of regularization\n",
    "c = 1000\n",
    "\n",
    "lr_classifiers = []\n",
    "\n",
    "for i in range(numb_datasets):\n",
    "    lr_classifiers.append(LogisticRegression(C=c))\n",
    "    lr_classifiers[i].fit(final_datasets[i]['train_X'], final_datasets[i]['train_Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Score each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if training_size != 1:\n",
    "    for i in range(numb_datasets):\n",
    "        score = lr_classifiers[i].score(final_datasets[i]['validation_X'], final_datasets[i]['validation_Y'])\n",
    "        predictions = lr_classifiers[i].predict(final_datasets[i]['validation_X'])\n",
    "        matrix = confusion_matrix(final_datasets[i]['validation_Y'], predictions)\n",
    "        print('Score for classifier #{0}: {1}'.format(i, score))\n",
    "        print('TN: {0}\\tFN: {1}\\tTP: {2}\\tFP:{3}'.format(matrix[0][0], matrix[1][0], matrix[1][1], matrix[0][1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check how each classifier deals with the target sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02391248  0.97608752]]\n",
      "[[ 0.00406816  0.99593184]]\n",
      "[[ 0.00108669  0.99891331]]\n",
      "[[ 0.00124398  0.99875602]]\n",
      "[[ 0.00627197  0.99372803]]\n",
      "[[ 0.00416493  0.99583507]]\n"
     ]
    }
   ],
   "source": [
    "# Malware sample\n",
    "target_link = 'MzIwZDgzMjY0YjQ5NGQxMjhkZjk1YjE0YTlkNGQ1OTE'\n",
    "# Goodware sample\n",
    "# target_link = 'ZmU4NWM2NDA2Y2VkNGU1YTljYzNkYjJhNmFhZGE1Mzg'\n",
    "target_imports = ';'.join(jcfg_data_loading.parse_static_imports(target_link))\n",
    "\n",
    "for i in range(numb_datasets):\n",
    "    X = (count_vec_array[i].transform([target_imports])).toarray()\n",
    "    print(lr_classifiers[i].predict_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[371,  90],\n",
       "       [ 95, 369]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = lr_classifiers[0].predict(count_vec_array[0].transform(pd.concat(datasets[1:]).imports).toarray())\n",
    "print(lr_classifiers[0].score(count_vec_array[0].transform(pd.concat(datasets[1:]).imports).toarray(), pd.concat(datasets[1:]).malware.values))\n",
    "confusion_matrix(pd.concat(datasets[1:]).malware.values, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
